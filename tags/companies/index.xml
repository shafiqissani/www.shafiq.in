<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shafiq Alibhai </title>
    <link>https://shafiqissani.github.io/www.shafiq.in/tags/companies/</link>
    <language>en-us</language>
    <author></author>
    <rights>(C) 2016</rights>
    <updated>2011-11-11 11:18:11 &#43;0000 UTC</updated>

    
      
        <item>
          <title>Applications of Neural Networks</title>
          <link>https://shafiqissani.github.io/www.shafiq.in/2011/11/11/applications-of-neural-networks/</link>
          <pubDate>Fri, 11 Nov 2011 11:18:11 UTC</pubDate>
          <author></author>
          <guid>https://shafiqissani.github.io/www.shafiq.in/2011/11/11/applications-of-neural-networks/</guid>
          <description>&lt;p align=&#34;CENTER&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;Download Link :Â &lt;a href=&#34;http://shafiqissani.files.wordpress.com/2011/11/2-appliations.doc&#34;&gt;Appliations&lt;/a&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Chapter 1. Introduction&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;Introduction to neural networks&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;1.1 What is a Neural Network?&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;An &lt;a class=&#34;zem_slink&#34; title=&#34;Artificial neural network&#34; href=&#34;http://en.wikipedia.org/wiki/Artificial_neural_network&#34; rel=&#34;wikipedia&#34;&gt;Artificial Neural Network&lt;/a&gt; (ANN) is an &lt;a class=&#34;zem_slink&#34; title=&#34;Information processing&#34; href=&#34;http://en.wikipedia.org/wiki/Information_processing&#34; rel=&#34;wikipedia&#34;&gt;information processing&lt;/a&gt; paradigm that is inspired by the way biological &lt;a class=&#34;zem_slink&#34; title=&#34;Nervous system&#34; href=&#34;http://en.wikipedia.org/wiki/Nervous_system&#34; rel=&#34;wikipedia&#34;&gt;nervous systems&lt;/a&gt;, such as the brain, process information. The key element of this paradigm is the novel structure of the information processing system. It is composed of a &lt;a class=&#34;zem_slink&#34; title=&#34;Large numbers&#34; href=&#34;http://en.wikipedia.org/wiki/Large_numbers&#34; rel=&#34;wikipedia&#34;&gt;large number&lt;/a&gt; of highly interconnected processing elements (neurons) working in unison to solve specific problems. ANNs, like people, learn by example. An ANN is configured for a specific application, such as pattern recognition or data classification, through a &lt;a class=&#34;zem_slink&#34; title=&#34;Learning&#34; href=&#34;http://en.wikipedia.org/wiki/Learning&#34; rel=&#34;wikipedia&#34;&gt;learning process&lt;/a&gt;. Learning in biological systems involves adjustments to the synaptic connections that exist between the neurons. This is true of ANNs as well. &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;1.2 Why use neural networks?&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;a class=&#34;zem_slink&#34; title=&#34;Neural network&#34; href=&#34;http://en.wikipedia.org/wiki/Neural_network&#34; rel=&#34;wikipedia&#34;&gt;Neural networks&lt;/a&gt;, with their remarkable ability to derive meaning from complicated or imprecise data, can be used to extract patterns and detect trends that are too complex to be noticed by either humans or other computer techniques. A trained neural network can be thought of as an &#34;expert&#34; in the category of information it has been given to analyze. This expert can then be used to provide projections given new situations of interest and answer &#34;what if&#34; questions.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;br /&gt;
Other advantages include: &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;Adaptive learning: An ability to learn how to do tasks based on the data given for training or initial experience. &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;a class=&#34;zem_slink&#34; title=&#34;Self-organization&#34; href=&#34;http://en.wikipedia.org/wiki/Self-organization&#34; rel=&#34;wikipedia&#34;&gt;Self-Organization&lt;/a&gt;: An ANN can create its own organization or representation of the information it receives during learning time. &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;Real Time Operation: ANN computations may be carried out in parallel, and special hardware devices are being designed and manufactured which take advantage of this capability. &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;a class=&#34;zem_slink&#34; title=&#34;Fault-tolerant system&#34; href=&#34;http://en.wikipedia.org/wiki/Fault-tolerant_system&#34; rel=&#34;wikipedia&#34;&gt;Fault Tolerance&lt;/a&gt; via Redundant Information Coding: Partial destruction of a network leads to the corresponding degradation of performance. However, some network capabilities may be retained even with major network damage. &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;1.3 Neural networks versus conventional computers&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;Neural networks take a different approach to &lt;a class=&#34;zem_slink&#34; title=&#34;Problem solving&#34; href=&#34;http://en.wikipedia.org/wiki/Problem_solving&#34; rel=&#34;wikipedia&#34;&gt;problem solving&lt;/a&gt; than that of conventional computers. Conventional computers use an algorithmic approach i.e. the computer follows a set of instructions in order to solve a problem. Unless the specific steps that the computer needs to follow are known the computer cannot solve the problem. That restricts the problem solving capability of conventional computers to problems that we already understand and know how to solve. But computers would be so much more useful if they could do things that we don&#39;t exactly know how to do. &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;Neural networks process information in a similar way the human brain does. The network is composed of a large number of highly interconnected processing elements (neurons) working in parallel to solve a specific problem. Neural networks learn by example. They cannot be programmed to perform a specific task. The examples must be selected carefully otherwise useful time is wasted or even worse the network might be functioning incorrectly. The disadvantage is that because the network finds out how to solve the problem by itself, its operation can be unpredictable.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;On the other hand, conventional computers use a cognitive approach to problem solving; the way the problem is to solved must be known and stated in small unambiguous instructions. These instructions are then converted to a high level language program and then into machine code that the computer can understand. These machines are totally predictable; if anything goes wrong is due to a software or hardware fault.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;Neural networks and conventional algorithmic computers are not in competition but complement each other. There are tasks are more suited to an algorithmic approach like arithmetic operations and tasks that are more suited to neural networks. Even more, a large number of tasks, require systems that use a combination of the two approaches (normally a conventional computer is used to supervise the neural network) in order to perform at maximum efficiency. &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;Neural networks do not perform miracles. But if used sensibly they can produce some amazing results.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;table width=&#34;608&#34; cellspacing=&#34;0&#34; cellpadding=&#34;1&#34;&gt;
&lt;tbody&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;577&#34; height=&#34;13&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;.1 Co Evolution of Neural Networks for Control of Pursuit &amp;amp; Evasion&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;3&#34; width=&#34;27&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;2&#34; width=&#34;582&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;The following MPEG movie sequences illustrate behavior generated by dynamical recurrent neural network controllers co-evolved for pursuit and evasion capabilities. From an initial population of random network designs, successful designs in each generation are selected for reproduction with recombination, mutation, and gene duplication. Selection is based on measures of how well each controller performs in a number of pursuit-evasion contests. In each contest a pursuer controller and an evader controller are pitched against each other, controlling simple ``visually guided&#39;&#39; 2-dimensional autonomous virtual agents. Both the pursuer and the evader have limited amounts of energy, which is used up in movement, so they have to evolve to move economically. Each contest results in a time-series of position and orientation data for the two agents.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;These time-series are then fed into a custom 3-D movie generator. It is important to note that, although the chase behaviors are genuine data, the 3D structures, surface physics, and shading are all purely for illustrative effect.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;CENTER&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_a1d2c35.png&#34; alt=&#34;&#34; width=&#34;251&#34; height=&#34;174&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;table width=&#34;464&#34; cellspacing=&#34;0&#34; cellpadding=&#34;0&#34;&gt;
&lt;col width=&#34;464&#34; /&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;464&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;1. The pursuer is not very good at pursuing, and the evader is not very good at evading.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;2. Pursuer chases evader, but soon runs out of energy, allowing the evader to escape.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;3. Pursuer chases evader, but uses up all its energy just before the evader runs out of energy.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;4. After a couple of close shaves, the pursuer finally catches the evader.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;2&#34; width=&#34;22&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;table style=&#34;font-family:&#39;Times New Roman&#39;;&#34; width=&#34;552&#34; cellspacing=&#34;0&#34; cellpadding=&#34;2&#34;&gt;
&lt;col width=&#34;22&#34; /&gt;
&lt;col width=&#34;522&#34; /&gt;
&lt;tbody&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;22&#34; height=&#34;20&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;2.2&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;522&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;Learning the Distribution of Object Trajectories for Event Recognition&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;div style=&#34;font-family:&#39;Times New Roman&#39;;line-height:normal;font-size:medium;&#34; align=&#34;RIGHT&#34;&gt;
&lt;table width=&#34;652&#34; cellspacing=&#34;0&#34; cellpadding=&#34;2&#34;&gt;
&lt;col width=&#34;316&#34; /&gt;
&lt;col width=&#34;2&#34; /&gt;
&lt;col width=&#34;322&#34; /&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;3&#34; valign=&#34;TOP&#34; width=&#34;648&#34; height=&#34;205&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;margin-left:1.23cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;This research work is about the modeling of object behaviors using detailed, learnt statistical models. The techniques being developed will allow models of characteristic object behaviors to be learnt from the continuous observation of long image sequences. It is hoped that these models of characteristic behaviors will have a number of uses, particularly in automated surveillance and event recognition, allowing the surveillance problem to be approached from a lower level, without the need for high-level scene/behavioral knowledge. Other possible uses include the random generation of realistic looking object behavior for use in Virtual Reality, and long-term prediction of object behaviors to aid occlusion reasoning in object tracking.Â &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;316&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;1. The model is learnt in an unsupervised manner by tracking objects over long image sequences, and is based on a combination of a neural network implementing VectorÂ &lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;Quantization and a type of neuron with short-term memory capabilities.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/jn-learn.mpg&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_33aea223.jpg&#34; alt=&#34;&#34; width=&#34;240&#34; height=&#34;223&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;&lt;br /&gt;
&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/jn-learn.mpg&#34; target=&#34;_blank&#34;&gt;1. Learning mode&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;2&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;322&#34;&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;2. Models of the trajectories of pedestrians have been generated and used to assess the typicality of new trajectories (allowing the identification of `incidents of interest&#39; within the scene), predict future object trajectories, and randomly generate new trajectories.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/jn-pred.mpg&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_m221deb5e.jpg&#34; alt=&#34;&#34; width=&#34;242&#34; height=&#34;228&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;&lt;br /&gt;
&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/jn-pred.mpg&#34; target=&#34;_blank&#34;&gt;2. Predict mode&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;table style=&#34;font-family:&#39;Times New Roman&#39;;&#34; width=&#34;626&#34; cellspacing=&#34;0&#34; cellpadding=&#34;2&#34;&gt;
&lt;col width=&#34;36&#34; /&gt;
&lt;col width=&#34;276&#34; /&gt;
&lt;col width=&#34;86&#34; /&gt;
&lt;col width=&#34;212&#34; /&gt;
&lt;tbody&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;36&#34; height=&#34;15&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;Â &lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;2.3&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;2&#34; width=&#34;366&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;Radiosity for Virtual Reality Systems (ROVER)&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;212&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;4&#34; valign=&#34;TOP&#34; width=&#34;622&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;The synthesis of actual and computer generated photo-realistic images has been the aim of artists and graphic designers for many decades. Some of the most realistic images (seeÂ &lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/rover/gg0.htm&#34;&gt;Graphics Gallery&lt;/a&gt;Â - simulated steel mill) were generated using radiosity techniques. Unlike ray tracing, radiosity models the actual interaction between the lights and the environment. In photo realistic Virtual Reality (VR) environments, the need for quick feedback based on user actions is crucial. It is generally recognised that traditional implementation of radiosity is computationally very expensive and therefore not feasible for use in VR systems where practical data sets are of huge complexity. In the original thesis, we introduce two new methods and several hybrid techniques to the radiosity research community on using radiosity in VR applications.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;On theÂ &lt;a style=&#34;color:#0000ff;&#34; href=&#34;%5Cshafiqissani_bkp%5CTask%5CcontentDevelopment%5Cstudent%20seminar%5CNeural%20Network%20Applications.htm#Flyby&#34;&gt;left&lt;/a&gt;Â column, flyby, walkthrough and a virtual space are first introduced and on the left. On theÂ &lt;a style=&#34;color:#0000ff;&#34; href=&#34;%5Cshafiqissani_bkp%5CTask%5CcontentDevelopment%5Cstudent%20seminar%5CNeural%20Network%20Applications.htm#ROVER%20Learning&#34;&gt;right&lt;/a&gt;, we showcase one of the two novel methods which were proposed using Neural Network technology.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;2&#34; width=&#34;316&#34;&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a name=&#34;Flyby&#34;&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;Introduction to Flyby, Walkthrough and Virtual Space&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_m7211bcfb.jpg&#34; alt=&#34;&#34; width=&#34;283&#34; height=&#34;148&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/ty-flyby.mpg&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;&lt;br /&gt;
Flyby&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/ty-walk.avi&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_363f2a73.jpg&#34; alt=&#34;&#34; width=&#34;285&#34; height=&#34;232&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;&lt;br /&gt;
&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/ty-walk.avi&#34; target=&#34;_blank&#34;&gt;3D Walkthrough&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/ty-offvr.mov&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_78ae7f87.jpg&#34; alt=&#34;&#34; width=&#34;295&#34; height=&#34;183&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;&lt;br /&gt;
&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/ty-offvr.mov&#34; target=&#34;_blank&#34;&gt;Virtual Space&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;2&#34; width=&#34;302&#34;&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a name=&#34;ROVER_Learning&#34;&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;(A) ROVER Learning from Examples&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_24623e01.jpg&#34; alt=&#34;&#34; width=&#34;277&#34; height=&#34;246&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/ty-walk1.mov&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;Sequence 1&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/ty-walk5.mov&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;Sequence 5&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/ty-walk8.mov&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;Sequence 8&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_419111f1.gif&#34; alt=&#34;&#34; width=&#34;624&#34; height=&#34;2&#34; align=&#34;BOTTOM&#34; /&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;(B) ROVER Modeling&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_13865add.png&#34; alt=&#34;&#34; width=&#34;281&#34; height=&#34;177&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_419111f1.gif&#34; alt=&#34;&#34; width=&#34;624&#34; height=&#34;2&#34; align=&#34;BOTTOM&#34; /&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;(C) ROVER Prediction&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_35b61065.png&#34; alt=&#34;&#34; width=&#34;281&#34; height=&#34;128&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;table style=&#34;font-family:&#39;Times New Roman&#39;;&#34; width=&#34;469&#34; cellspacing=&#34;0&#34; cellpadding=&#34;2&#34;&gt;
&lt;col width=&#34;35&#34; /&gt;
&lt;col width=&#34;427&#34; /&gt;
&lt;tbody&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;35&#34; height=&#34;1&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;2.4&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;427&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a name=&#34;Autonomous_Walker_&amp;amp;_Swimming_Eel&#34;&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;Autonomous Walker &amp;amp; Swimming Eel&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;div style=&#34;font-family:&#39;Times New Roman&#39;;line-height:normal;font-size:medium;&#34; align=&#34;RIGHT&#34;&gt;
&lt;table width=&#34;626&#34; cellspacing=&#34;0&#34; cellpadding=&#34;2&#34;&gt;
&lt;col width=&#34;311&#34; /&gt;
&lt;col width=&#34;2&#34; /&gt;
&lt;col width=&#34;302&#34; /&gt;
&lt;tbody&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;311&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;(A) The research in this area involves combining biology, mechanical engineering and information technology in order to develop the techniques necessary to build a dynamically stable legged vehicle controlled by a neural network. This would incorporate command signals, sensory feedback and reflex circuitry in order to produce the desired movement.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/al-climb.mpg&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_m14f5d309.png&#34; alt=&#34;&#34; width=&#34;290&#34; height=&#34;217&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;&lt;br /&gt;
&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/al-climb.mpg&#34; target=&#34;_blank&#34;&gt;Walker&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;2&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;302&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;(B) Simulation of the swimming lamprey (eel-like sea creature), driven by a neural network.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/al-lamp.mpg&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_2ac09d1a.jpg&#34; alt=&#34;&#34; width=&#34;282&#34; height=&#34;195&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;&lt;br /&gt;
&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/al-lamp.mpg&#34; target=&#34;_blank&#34;&gt;Swimming Lamprey&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_419111f1.gif&#34; alt=&#34;&#34; width=&#34;624&#34; height=&#34;2&#34; align=&#34;BOTTOM&#34; /&gt;&lt;/p&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;table style=&#34;font-family:&#39;Times New Roman&#39;;&#34; width=&#34;594&#34; cellspacing=&#34;0&#34; cellpadding=&#34;2&#34;&gt;
&lt;col width=&#34;75&#34; /&gt;
&lt;col width=&#34;511&#34; /&gt;
&lt;tbody&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;75&#34; height=&#34;15&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;2.5&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;511&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a name=&#34;Robocup:_Robot_World_Cup&#34;&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;Robocup: Robot World Cup&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;2&#34; valign=&#34;TOP&#34; width=&#34;590&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;The RoboCup Competition pits robots (real and virtual) against each other in a simulated soccer tournament. The aim of the RoboCup competition is to foster an interdisciplinary approach to robotics and agent-based AI by presenting a domain that requires large-scale coorperation and coordination in a dynamic, noisy, complex environment.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;RoboCup has three different leagues to-date. The Small and Middle-Size Leagues involved physical robots; the Simulation League is for virtual, synthetic teams. This work focus on building softbots for the Simulation League.&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;Machine Learning for Robocup involves:&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.18cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;The training of player in the process of making the decision of whether (a) to dribble the ball; (b) to pass it on to another team-mate; (c) to shoot into the net.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.18cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;The training of the goalkeeper in process of intelligent guessing of how the ball is going to be kick by the opponents. Complexities arise when one opponent decides to pass the ball to another player instead of attempting a score.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.18cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;Evolution of a co-operative and perhaps unpredictable team.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;Common AI methods used are variants of Neural Networks and Genetic Algorithms.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;2&#34; valign=&#34;TOP&#34; width=&#34;590&#34;&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/sp-robo.avi&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_70ade2d3.png&#34; alt=&#34;&#34; width=&#34;475&#34; height=&#34;376&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;br /&gt;
&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/sp-robo.avi&#34; target=&#34;_blank&#34;&gt;KRDL Soccer Softbots&lt;/a&gt;Â (3.1mb, AVI)&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_419111f1.gif&#34; alt=&#34;&#34; width=&#34;624&#34; height=&#34;2&#34; align=&#34;BOTTOM&#34; /&gt;&lt;/p&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;table style=&#34;font-family:&#39;Times New Roman&#39;;&#34; width=&#34;487&#34; cellspacing=&#34;0&#34; cellpadding=&#34;2&#34;&gt;
&lt;col width=&#34;30&#34; /&gt;
&lt;col width=&#34;449&#34; /&gt;
&lt;tbody&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;30&#34; height=&#34;8&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;2.6&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;449&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a name=&#34;Using_HMM&#39;s_for_Audio-to-Visual_Conversi&#34;&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;Using HMM&#39;s for Audio-to-Visual Conversion&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;div style=&#34;font-family:&#39;Times New Roman&#39;;line-height:normal;font-size:medium;&#34; align=&#34;RIGHT&#34;&gt;
&lt;table width=&#34;594&#34; cellspacing=&#34;0&#34; cellpadding=&#34;2&#34;&gt;
&lt;col width=&#34;278&#34; /&gt;
&lt;col width=&#34;2&#34; /&gt;
&lt;col width=&#34;302&#34; /&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;3&#34; valign=&#34;TOP&#34; width=&#34;590&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;One emerging application which exploits the correlation between audio and video is speech-driven facial animation. The goal of speech-driven facial animation is to synthesize realistic video sequences from acoustic speech. Much of the previous research has implemented this audio-to-visual conversion strategy with existing techniques such as vector quantization and neural networks. Here, they examine how this conversion process can be accomplished with hidden Markov models (HMM).&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;278&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;(A) Tracking Demo: The parabolic contour is fit to each frame of the video sequence using a modified deformable template algorithm. The height between the two contours, and the width between the corners of the mouth can be extracted from the templates to form our visual parameter sets.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/rr-track.avi&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_m3a4014d3.jpg&#34; alt=&#34;&#34; width=&#34;139&#34; height=&#34;108&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;&lt;br /&gt;
&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/rr-track.avi&#34; target=&#34;_blank&#34;&gt;Tracking&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;2&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;302&#34;&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;(B) Morphing Demo: Another important piece of the speech-driven facial animation system is a visual synthesis module. Here we are attempting to synthesize the word &#34;wow&#34; from a single image. Each frame in the video sequence is morphed from the first frame shown below. The parameters used to morph these images were obtained by hand.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/rr-morph.avi&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_3c21d6f1.jpg&#34; alt=&#34;&#34; width=&#34;139&#34; height=&#34;108&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;&lt;br /&gt;
&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/rr-morph.avi&#34; target=&#34;_blank&#34;&gt;Morphing&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_419111f1.gif&#34; alt=&#34;&#34; width=&#34;624&#34; height=&#34;2&#34; align=&#34;BOTTOM&#34; /&gt;&lt;/p&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;table style=&#34;font-family:&#39;Times New Roman&#39;;&#34; width=&#34;594&#34; cellspacing=&#34;0&#34; cellpadding=&#34;2&#34;&gt;
&lt;col width=&#34;265&#34; /&gt;
&lt;col width=&#34;321&#34; /&gt;
&lt;tbody&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;265&#34; height=&#34;15&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a name=&#34;Artifical_Life:_Galapagos&#34;&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;2.7 Artificial Life: Galapagos&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;321&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;2&#34; valign=&#34;TOP&#34; width=&#34;590&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;Galapagos is a fantastic and dangerous place where up and down have no meaning, where rivers of iridescent acid and high-energy laser mines are beautiful but deadly artifacts of some other time. Through spatially twisted puzzles and bewildering cyber-landscapes, the&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;Â artificial creature called Mendel struggles to survive, and you must help him.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Mendel is a synthetic organism that can sense infrared radiation and tactile stimulus. His mind is an advanced adaptive controller featuring Non-stationary Entropic Reduction Mapping -- a new form of artificial life technology developed by Anark. He can learn like your dog, he can adapt to hostile environments like a cockroach, but he can&#39;t solve the puzzles that prevent his escape from Galapagos.&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;265&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;Galapagos features rich, 3D texture-mapped worlds, with continuous-motion graphics and 6 degrees of freedom. Dramatic camera movement and incredible lighting effects make your passage through Galapagos breathtaking. Explosions and other chilling effects will make you fear for your synthetic friend. Active panning 3D stereo sound will draw you into the exotic worlds of Galapagos.&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;321&#34;&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/galapago.mov&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_m57c9a6cd.jpg&#34; alt=&#34;&#34; width=&#34;300&#34; height=&#34;183&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;br /&gt;
&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/galapago.mov&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;Galapagos&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_419111f1.gif&#34; alt=&#34;&#34; width=&#34;624&#34; height=&#34;2&#34; align=&#34;BOTTOM&#34; /&gt;&lt;/p&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;table style=&#34;font-family:&#39;Times New Roman&#39;;&#34; width=&#34;615&#34; cellspacing=&#34;0&#34; cellpadding=&#34;2&#34;&gt;
&lt;col width=&#34;294&#34; /&gt;
&lt;col width=&#34;4&#34; /&gt;
&lt;col width=&#34;2&#34; /&gt;
&lt;col width=&#34;299&#34; /&gt;
&lt;tbody&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;294&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a name=&#34;Speechreading_(Lipreading)&#34;&gt;&lt;/a&gt;Â &lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;2.8 Speechreading (Lipreading)&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;4&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;2&#34; width=&#34;305&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;4&#34; valign=&#34;TOP&#34; width=&#34;611&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;As part of the research program Neuroinformatik the IPVR develops a neural speechreading system as part of a user interface for a workstation. The three main parts of the system include a face tracker (done by Marco Sommerau), lip modeling and speech processing (done by Michael Vogt) and the development and application of SNNS for neural network training (done by GÃ¼nter Mamier).&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;Automatic speechreading is based on a robust lip image analysis. In this approach, no special illumination or lip make-up is used. The analysis is based on true color video images. The system allows for realtime tracking and storage of the lip region and robust off-line lip model matching. The proposed model is based on cubic outline curves. A neural classifier detects visibility of teeth edges and other attributes. At this stage of the approach the edge between the closed lips is automatically modeled if applicable, based on a neural network&#39;s decision.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To achieve high flexibility during lip-model development, a model description language has been defined and implemented. The language allows the definition of edge models (in general) based on knots and edge functions. Inner model forces stabilize the overall model shape. User defined image processing functions may be applied along the model edges. These functions and the inner forces contribute to an overall energy function. Adaptation of the model is done by gradient descent or simulated annealing like algorithms. The figure shows one configuration of the lip model, consisting of an upper lip edge and a lower lip edge. The model edges are defined by Bezier-functions. Outer control knots stabilize the position of the corners of the mouth.&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;2&#34; width=&#34;302&#34;&gt;
&lt;table width=&#34;302&#34; cellspacing=&#34;0&#34; cellpadding=&#34;0&#34;&gt;
&lt;col width=&#34;302&#34; /&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;302&#34;&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_4b5852f7.png&#34; alt=&#34;&#34; width=&#34;281&#34; height=&#34;208&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;302&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;Fig 2.8.1 The model interpreter enables a permanent measurement of model knot positions and color blends along model edges during adaptation to an utterance. The resulting parameters may be used for speech recognition tasks in further steps.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;2&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;299&#34;&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:16px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/gm-lip.mpg&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_m17b1bc31.jpg&#34; alt=&#34;&#34; width=&#34;279&#34; height=&#34;195&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;br /&gt;
&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/gm-lip.mpg&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;Lipread&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_419111f1.gif&#34; alt=&#34;&#34; width=&#34;624&#34; height=&#34;2&#34; align=&#34;BOTTOM&#34; /&gt;&lt;/p&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;table style=&#34;font-family:&#39;Times New Roman&#39;;&#34; width=&#34;606&#34; cellspacing=&#34;0&#34; cellpadding=&#34;2&#34;&gt;
&lt;col width=&#34;591&#34; /&gt;
&lt;col width=&#34;7&#34; /&gt;
&lt;tbody&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;591&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a name=&#34;Detection_and_Tracking_of_Moving_Targets&#34;&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;2.9 Detection and Tracking of Moving Targets&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;7&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;2&#34; valign=&#34;TOP&#34; width=&#34;602&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;The moving target detection and track methods here are &#34;track before detect&#34; methods. They correlate sensor data versus time and location, based on the nature of actual tracks. The track statistics are &#34;learned&#34; based on artificial neural network (ANN) training with prior real or simulated data. Effects of different clutter backgrounds are partially compensated based on space-time-adaptive processing of the sensor inputs, and further compensated based on the ANN training. Specific processing structures are adapted to the target track statistics and sensor characteristics of interest. Fusion of data over multiple wavelengths and sensors is also supported.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Compared to conventional fixed matched filter techniques, these methods have been shown to reduce false alarm rates by up to a factor of 1000 based on simulated SBIRS data for very weak ICBM targets against cloud and nuclear backgrounds, with photon, quantization, and thermal noise, and sensor jitter included. Examples of the backgrounds, and processing results, are given below.&lt;/p&gt;
&lt;p&gt;The methods are designed to overcome the weaknesses of other advanced track-before-detect methods, such as 3+-D (space, time, etc.) matched filtering, dynamic programming (DP), and multi-hypothesis tracking (MHT). Loosely speaking, 3+-D matched filtering requires too many filters in practice for long-term track correlation; DP cannot realistically exploit the non-Markovian nature of real tracks, and strong targets mask out weak targets; and MHT cannot support the low pre-detection thresholds required for very weak targets in high clutter. They have developed and tested versions of the above (and other) methods in their research, as well as Kalman-filter probabilistic data association (KF/PDA) methods, which they use for post-detection tracking.&lt;/p&gt;
&lt;p&gt;Space-time-adaptive methods are used to deal with correlated, non-stationary, non-Gaussian clutter, followed by a multi-stage filter sequence and soft-thresholding units that combine current and prior sensor data, plus feed back of prior outputs, to estimate the probability of target presence. The details are optimized by adaptive &#34;training&#34; over very large data sets, and special methods are used to maximize the efficiency of this training.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;2&#34; valign=&#34;TOP&#34; width=&#34;602&#34;&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/dgi-trk.mpg&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_47da4f29.jpg&#34; alt=&#34;&#34; width=&#34;479&#34; height=&#34;144&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;br /&gt;
Figure 2.9 (a) Raw input backgrounds with weak targets included,&lt;br /&gt;
(b) Detected target sequence at the ANN processing output,&lt;br /&gt;
post-detection tracking not included.Â &lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/dgi-trk.mpg&#34; target=&#34;_blank&#34;&gt;Video Clip&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;table style=&#34;font-family:&#39;Times New Roman&#39;;&#34; width=&#34;594&#34; cellspacing=&#34;0&#34; cellpadding=&#34;2&#34;&gt;
&lt;col width=&#34;291&#34; /&gt;
&lt;col width=&#34;2&#34; /&gt;
&lt;col width=&#34;117&#34; /&gt;
&lt;col width=&#34;135&#34; /&gt;
&lt;col width=&#34;30&#34; /&gt;
&lt;tbody&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;3&#34; width=&#34;417&#34; height=&#34;15&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a name=&#34;Real-time_Target_Identification_for_Secu&#34;&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;2.10 Real-time Target Identification for Security Applications&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;135&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;30&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;5&#34; valign=&#34;TOP&#34; width=&#34;590&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;The system localizes and tracks peoples&#39; faces as they move through a scene. It integrates the following techniques:&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.18cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;Motion detection&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.18cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;Tracking people based upon motion&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.18cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;Tracking faces using an appearance model&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;Faces are tracked robustly by integrating motion and model-based tracking.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;291&#34;&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;(A) Tracking in low resolution and poor lighting conditions&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/sm-jon.mpg&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_68e9ab48.jpg&#34; alt=&#34;&#34; width=&#34;171&#34; height=&#34;132&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;br /&gt;
&lt;span style=&#34;font-size:x-small;&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/sm-jon.mpg&#34; target=&#34;_blank&#34;&gt;Jon&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;2&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;3&#34; width=&#34;290&#34;&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;(B) Tracking two people simultaneously: lock is maintained on the faces despite unreliable motion-based body tracking.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/sm-2ppl.mpg&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_5a8c48dd.jpg&#34; alt=&#34;&#34; width=&#34;171&#34; height=&#34;132&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;br /&gt;
&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/sm-2ppl.mpg&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;Double Tracking&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;table style=&#34;font-family:&#39;Times New Roman&#39;;&#34; width=&#34;606&#34; cellspacing=&#34;0&#34; cellpadding=&#34;2&#34;&gt;
&lt;col width=&#34;57&#34; /&gt;
&lt;col width=&#34;541&#34; /&gt;
&lt;tbody&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;57&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;2.11&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;541&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a name=&#34;Facial_Animation&#34;&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;Facial Animation&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;2&#34; valign=&#34;TOP&#34; width=&#34;602&#34;&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;Facial animations created using hierarchical B-spline as the underlying surface representation. Neural networks could be use for learning of each variation in the face expressions for animated sequences.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.18cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;The (mask) model was created in SoftImage, and is an early prototype for the character &#34;Mouse&#34; in the YTV/ABC televisions series &#34;ReBoot&#34; (They do not use hierarchical splines for Reboot!). The original standard bicubic B-spline was imported to the &#34;Dragon&#34; editor and a hierarchy automatically constructed. The surface was attached to a jaw to allow it to open and close the mouth. Groups of control vertices were then moved around to created various facial expressions. Three of these expressions were chosen as key shapes, the spline surface was exported back to SoftImage, and the key shapes were interpolated to create the final animation.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;table width=&#34;542&#34; cellspacing=&#34;0&#34; cellpadding=&#34;0&#34;&gt;
&lt;col width=&#34;271&#34; /&gt;
&lt;col width=&#34;271&#34; /&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;271&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/df-mask.mpg&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_m420292cc.jpg&#34; alt=&#34;&#34; width=&#34;211&#34; height=&#34;162&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;&lt;br /&gt;
&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/df-mask.mpg&#34; target=&#34;_blank&#34;&gt;Mask&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;271&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/df-haida.mpg&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_27f46fd2.jpg&#34; alt=&#34;&#34; width=&#34;207&#34; height=&#34;162&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;&lt;br /&gt;
&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/haida.mpg&#34; target=&#34;_blank&#34;&gt;Haida&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;table style=&#34;font-family:&#39;Times New Roman&#39;;&#34; width=&#34;646&#34; cellspacing=&#34;0&#34; cellpadding=&#34;2&#34;&gt;
&lt;col width=&#34;40&#34; /&gt;
&lt;col width=&#34;106&#34; /&gt;
&lt;col width=&#34;150&#34; /&gt;
&lt;col width=&#34;150&#34; /&gt;
&lt;col width=&#34;135&#34; /&gt;
&lt;col width=&#34;41&#34; /&gt;
&lt;tbody&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;40&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;2.12&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;4&#34; width=&#34;553&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a name=&#34;Artificial_Life_for_Graphics,_Animation,&#34;&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;Artificial Life for Graphics, Animation, Multimedia, and Virtual Reality&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;41&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;6&#34; valign=&#34;TOP&#34; width=&#34;642&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;Some graphics researchers have begun to explore a new frontier--a world of objects of enormously greater complexity than is typically accessible through physical modeling alone--objects that are alive. The modeling and simulation of living systems for computer graphics resonates with the burgeoning field of scientific inquiry called Artificial Life. Conceptually, artificial life transcends the traditional boundaries of computer science and biological science. The natural synergy between computer graphics and artificial life can be potentially beneficial to both disciplines. As some of the demos here demonstrate, potential is becoming fulfillment.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The demos demonstrate and elucidate new models that realistically emulate a broad variety of living things--both plants and animals--from lower animals all the way up the evolutionary ladder to humans. Typically, these models inhabit virtual worlds in which they are subject to physical laws. Consequently, they often make use of physics-based modeling techniques. More significantly, however, they must also simulate many of the natural processes that uniquely characterize living systems--such as birth and death, growth, natural selection, evolution, perception, locomotion, manipulation, adaptive behavior, intelligence, and learning. The challenge is to develop sophisticated graphics models that are self-creating, self-evolving, self-controlling, and/or self-animating by simulating the natural mechanisms fundamental to life.&lt;/p&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;2&#34; width=&#34;150&#34;&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;CENTER&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/sg95-dog.mov&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_m35992294.jpg&#34; alt=&#34;&#34; width=&#34;129&#34; height=&#34;99&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;&lt;br /&gt;
&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/sg95-dog.mov&#34; target=&#34;_blank&#34;&gt;A.Dog&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;150&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;CENTER&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/sg95-evc.mpg&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_m6641c0d.jpg&#34; alt=&#34;&#34; width=&#34;129&#34; height=&#34;93&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;&lt;br /&gt;
&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/sg95-evc.mpg&#34; target=&#34;_blank&#34;&gt;Evolved Virtual Creatures&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;150&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;CENTER&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/sg95-sac.mov&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_404ed409.jpg&#34; alt=&#34;&#34; width=&#34;129&#34; height=&#34;100&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;&lt;br /&gt;
&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/sg95-sac.mov&#34; target=&#34;_blank&#34;&gt;Sensor-Based Autonomous Creatures&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;2&#34; width=&#34;179&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;CENTER&#34;&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/sg95-af.mov&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_m2187c499.jpg&#34; alt=&#34;&#34; width=&#34;129&#34; height=&#34;101&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;&lt;br /&gt;
&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/sg95-af.mov&#34; target=&#34;_blank&#34;&gt;A. Fish&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.35cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_419111f1.gif&#34; alt=&#34;&#34; width=&#34;624&#34; height=&#34;2&#34; align=&#34;BOTTOM&#34; /&gt;&lt;/p&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;
&lt;table style=&#34;font-family:&#39;Times New Roman&#39;;&#34; width=&#34;606&#34; cellspacing=&#34;0&#34; cellpadding=&#34;2&#34;&gt;
&lt;col width=&#34;324&#34; /&gt;
&lt;col width=&#34;274&#34; /&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; colspan=&#34;2&#34; valign=&#34;TOP&#34; width=&#34;602&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;a name=&#34;Creatures:_The_World_Most_Advanced_Artif&#34;&gt;&lt;/a&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;strong&gt;2.13 Creatures: The World Most Advanced Artificial Life!&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;324&#34; height=&#34;225&#34;&gt;
&lt;p class=&#34;western&#34; style=&#34;margin-bottom:.21cm;direction:ltr;line-height:17px;font-family:Calibri, sans-serif;font-size:11pt;&#34; lang=&#34;en-US&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;Creatures is the most entertaining computer game you&#39;ll ever play which offers nothing to shoot, no puzzles to solve or difficult controls to master. And yet it isÂ &lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;mesmerizing entertainment.&lt;br /&gt;
One has to raise, teach, breed and love computer pets that are really alive. They are so alive that if it is not taken care of, they will die. Creaturesâ features the most advanced, genuine Artificial Life software ever developed in a commercial product, technology that has blown the imaginations of scientists world-wide. This is a look into the future where new species of life emerge from ordinary home and office PCs.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;274&#34;&gt;
&lt;p style=&#34;margin-bottom:.21cm;direction:ltr;line-height:18px;margin-top:.18cm;&#34; lang=&#34;en-US&#34; align=&#34;CENTER&#34;&gt;&lt;span style=&#34;color:#808080;&#34;&gt;&lt;img src=&#34;{{ site.baseurl }}/assets/2.%20Appliations_html_m59a26c52.jpg&#34; alt=&#34;&#34; width=&#34;253&#34; height=&#34;193&#34; align=&#34;BOTTOM&#34; border=&#34;0&#34; /&gt;&lt;/span&gt;&lt;a style=&#34;color:#0000ff;&#34; href=&#34;http://tralvex.com/pub/nap/video/creature.avi&#34; target=&#34;_blank&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;br /&gt;
Creatures&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr valign=&#34;TOP&#34;&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;324&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;border-color:initial;border-style:none;border-width:initial;padding:0;&#34; width=&#34;274&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;Â &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-bottom:0;&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;The following MPEG movie sequences illustrate behavior generated by dynamical recurrent neural network controllers co-evolved for pursuit and evasion capabilities. From an initial population of random network designs, successful designs in each generation are selected for reproduction with recombination, mutation, and gene duplication. Selection is based on measures of how well each controller performs in a number of pursuit-evasion contests. In each contest a pursuer controller and an evader controller are pitched against each other, controlling simple ``visually guided&#39;&#39; 2-dimensional autonomous virtual agents. Both the pursuer and the evader have limited amounts of energy, which is used up in movement, so they have to evolve to move economically. Each contest results in a time-series of position and orientation data for the two agents. &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-top:.18cm;margin-bottom:.18cm;line-height:100%;&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;These time-series are then fed into a custom 3-D movie generator. It is important to note that, although the chase behaviors are genuine data, the 3D structures, surface physics, and shading are all purely for illustrative effect. &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;table width=&#34;464&#34; cellspacing=&#34;0&#34; cellpadding=&#34;0&#34;&gt;
&lt;col width=&#34;464&#34; /&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;border:none;padding:0;&#34; width=&#34;464&#34;&gt;
&lt;p style=&#34;margin-bottom:0;&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;1. The pursuer is not very good at pursuing, and the evader is not very good at evading. &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-top:.18cm;margin-bottom:.18cm;&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;2. Pursuer chases evader, but soon runs out of energy, allowing the evader to escape. &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-top:.18cm;margin-bottom:.18cm;&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:small;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;3. Pursuer chases evader, but uses up all its energy just before the evader runs out of energy. &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p style=&#34;margin-top:.18cm;&#34; align=&#34;JUSTIFY&#34;&gt;&lt;span style=&#34;font-family:&#39;Times New Roman&#39;, serif;&#34;&gt;&lt;span style=&#34;font-size:x-small;&#34;&gt;4. After a couple of close shaves, the pursuer finally catches the evader. &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
        </item>
      
    

  </channel>
</rss>
